{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T19:43:27.468396Z",
     "start_time": "2025-11-15T19:43:26.364231Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install -r requirements.txt\n",
   "id": "8160c033793356b8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (2.2.2)\r\n",
      "Requirement already satisfied: torchvision in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (0.17.2)\r\n",
      "Requirement already satisfied: numpy==1.26.4 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (1.26.4)\r\n",
      "Requirement already satisfied: matplotlib in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (3.10.7)\r\n",
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (1.7.2)\r\n",
      "Requirement already satisfied: Pillow in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (12.0.0)\r\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from torch->-r requirements.txt (line 1)) (3.20.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.venv/lib/python3.10/site-packages (from torch->-r requirements.txt (line 1)) (4.15.0)\r\n",
      "Requirement already satisfied: sympy in ./.venv/lib/python3.10/site-packages (from torch->-r requirements.txt (line 1)) (1.14.0)\r\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.10/site-packages (from torch->-r requirements.txt (line 1)) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.10/site-packages (from torch->-r requirements.txt (line 1)) (3.1.6)\r\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.10/site-packages (from torch->-r requirements.txt (line 1)) (2025.10.0)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 4)) (1.3.2)\r\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 4)) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 4)) (4.60.1)\r\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 4)) (1.4.9)\r\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 4)) (25.0)\r\n",
      "Requirement already satisfied: pyparsing>=3 in ./.venv/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 4)) (3.2.5)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 4)) (2.9.0.post0)\r\n",
      "Requirement already satisfied: scipy>=1.8.0 in ./.venv/lib/python3.10/site-packages (from scikit-learn->-r requirements.txt (line 5)) (1.15.3)\r\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.10/site-packages (from scikit-learn->-r requirements.txt (line 5)) (1.5.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.10/site-packages (from scikit-learn->-r requirements.txt (line 5)) (3.6.0)\r\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->-r requirements.txt (line 4)) (1.17.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.10/site-packages (from jinja2->torch->-r requirements.txt (line 1)) (3.0.3)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.10/site-packages (from sympy->torch->-r requirements.txt (line 1)) (1.3.0)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.3\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T19:43:28.527680Z",
     "start_time": "2025-11-15T19:43:27.476637Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install split-folders",
   "id": "70438935e19e1275",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: split-folders in ./.venv/lib/python3.10/site-packages (0.5.1)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.3\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T19:43:28.536527Z",
     "start_time": "2025-11-15T19:43:28.533274Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Data split to test, train and validation subfolders\n",
    "# import os\n",
    "# import shutil\n",
    "# import random\n",
    "#\n",
    "# random.seed(42) # for reproducibility\n",
    "#\n",
    "# # Paths\n",
    "# input_dir = 'all_data'              # your source directory\n",
    "# output_base = 'data_split'          # where splits will go\n",
    "# splits = ['train', 'val', 'test']   # split folder names\n",
    "# split_ratios = {'train': 0.7, 'val': 0.15, 'test': 0.15}\n",
    "# classes = ['Covid', 'Normal']       # subfolder names\n",
    "#\n",
    "# # Create output directories\n",
    "# for split in splits:\n",
    "#     for cls in classes:\n",
    "#         split_path = os.path.join(output_base, split, cls)\n",
    "#         os.makedirs(split_path, exist_ok=True)\n",
    "#\n",
    "# for cls in classes:\n",
    "#     imgs = os.listdir(os.path.join(input_dir, cls))\n",
    "#     random.shuffle(imgs)\n",
    "#     n_total = len(imgs)\n",
    "#     n_train = int(n_total * split_ratios['train'])\n",
    "#     n_val = int(n_total * split_ratios['val'])\n",
    "#     n_test = n_total - n_train - n_val  # the rest\n",
    "#\n",
    "#     train_imgs = imgs[:n_train]\n",
    "#     val_imgs = imgs[n_train:n_train + n_val]\n",
    "#     test_imgs = imgs[n_train + n_val:]\n",
    "#\n",
    "#     for img_name, split in zip([train_imgs, val_imgs, test_imgs], splits):\n",
    "#         dst_dir = os.path.join(output_base, split, cls)\n",
    "#         for img in img_name:\n",
    "#             src = os.path.join(input_dir, cls, img)\n",
    "#             dst = os.path.join(dst_dir, img)\n",
    "#             shutil.copy2(src, dst)\n",
    "\n"
   ],
   "id": "38b16fe832020e9f",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T19:45:55.580790Z",
     "start_time": "2025-11-15T19:45:55.576271Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Importing the required libraries\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n"
   ],
   "id": "5cc046b1e0289bb6",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T19:43:30.658084Z",
     "start_time": "2025-11-15T19:43:30.654760Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Defining the transformations : image size and augmentation will depend on pre-trained model. Since, I am using VGG/ResNet, they expect 224 X 224 px inputs\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_test_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ],
   "id": "5441156ad615adee",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T19:43:30.670049Z",
     "start_time": "2025-11-15T19:43:30.662018Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Creating ImageFolders for each split\n",
    "\n",
    "train_dataset = datasets.ImageFolder('data_split/train', transform=train_transforms)\n",
    "val_dataset = datasets.ImageFolder('data_split/val', transform=val_test_transforms)\n",
    "test_dataset = datasets.ImageFolder('data_split/test', transform=val_test_transforms)\n"
   ],
   "id": "f4a39db0a7fc5a11",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T19:43:30.677734Z",
     "start_time": "2025-11-15T19:43:30.674984Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Creating DataLoaders for each split\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ],
   "id": "33fcd15ea6238a9a",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T19:43:30.682963Z",
     "start_time": "2025-11-15T19:43:30.681142Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import os\n",
    "# print(os.listdir('data_split/train/Covid'))\n",
    "# from PIL import Image\n",
    "# img = Image.open('data_split/train/Covid/COVID-19(134).png')\n",
    "# img.show()\n"
   ],
   "id": "5eaf55279b99b111",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T19:43:31.386437Z",
     "start_time": "2025-11-15T19:43:30.687507Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# check to see if batch is loading correctly\n",
    "images, labels = next(iter(train_loader))\n",
    "print('Image batch shape:', images.shape)\n",
    "print('Label batch shape:', labels.shape)\n"
   ],
   "id": "9e8753af05edcd64",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image batch shape: torch.Size([32, 3, 224, 224])\n",
      "Label batch shape: torch.Size([32])\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T19:46:08.432715Z",
     "start_time": "2025-11-15T19:45:58.961823Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Loading the pre-trained model and adapting the classifier\n",
    "\n",
    "model = models.resnet50(pretrained=True)\n",
    "\n",
    "# Freeze backbone parametersâ€”optional for transfer learning\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace the FC layer (classifier) for binary (or multi-class) output\n",
    "num_classes = 2  # covid/normal\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(model.fc.in_features, num_classes)\n",
    ")\n"
   ],
   "id": "1fbd648edec88d72",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rashila-lamichhane/PycharmProjects/JupyterProject/.venv/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/rashila-lamichhane/PycharmProjects/JupyterProject/.venv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /Users/rashila-lamichhane/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
      "24.0%IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "65.7%IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100.0%\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T19:46:55.932107Z",
     "start_time": "2025-11-15T19:46:55.923031Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Move model to device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)"
   ],
   "id": "1eb49fb66cbd628f",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T19:47:35.647830Z",
     "start_time": "2025-11-15T19:47:35.643819Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# setting up loss function and optimizer : Binary cross entropy loss works for image classification\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=1e-4)"
   ],
   "id": "24ad19dd3740c71e",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T20:03:26.186870Z",
     "start_time": "2025-11-15T19:47:59.145497Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    print(f'Epoch {epoch+1}, Loss: {running_loss/total}, Acc: {correct/total}')\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "    print(f'  Val Loss: {val_loss/val_total}, Val Acc: {val_correct/val_total}')\n"
   ],
   "id": "a7d313f629442706",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.47577372704838305, Acc: 0.7901008249312558\n",
      "  Val Loss: 0.4001378178148822, Val Acc: 0.8626609442060086\n",
      "Epoch 2, Loss: 0.365009196355297, Acc: 0.8597616865261228\n",
      "  Val Loss: 0.3192114137400885, Val Acc: 0.8626609442060086\n",
      "Epoch 3, Loss: 0.3147606367397265, Acc: 0.8606782768102658\n",
      "  Val Loss: 0.2685484538262494, Val Acc: 0.8626609442060086\n",
      "Epoch 4, Loss: 0.26924076873047653, Acc: 0.8716773602199817\n",
      "  Val Loss: 0.22671085078061395, Val Acc: 0.8798283261802575\n",
      "Epoch 5, Loss: 0.23852976924448685, Acc: 0.8835930339138405\n",
      "  Val Loss: 0.1901414079727533, Val Acc: 0.9012875536480687\n",
      "Epoch 6, Loss: 0.20626676492358872, Acc: 0.916590284142988\n",
      "  Val Loss: 0.16839470330609785, Val Acc: 0.9227467811158798\n",
      "Epoch 7, Loss: 0.18042185357630963, Acc: 0.9275893675527039\n",
      "  Val Loss: 0.14837407685272683, Val Acc: 0.9313304721030042\n",
      "Epoch 8, Loss: 0.16720448700006024, Acc: 0.9422548120989918\n",
      "  Val Loss: 0.12948200925225353, Val Acc: 0.9613733905579399\n",
      "Epoch 9, Loss: 0.1475535592731809, Acc: 0.9550870760769936\n",
      "  Val Loss: 0.11868193432241039, Val Acc: 0.9613733905579399\n",
      "Epoch 10, Loss: 0.1292720891323579, Acc: 0.9633363886342805\n",
      "  Val Loss: 0.10748834789694635, Val Acc: 0.9656652360515021\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T21:33:26.845516Z",
     "start_time": "2025-11-15T21:33:09.828424Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Put model in evaluation mode\n",
    "# 1. Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "test_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# 2. Disable gradient calculation (for efficiency)\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "avg_loss = test_loss / total\n",
    "acc = correct / total\n",
    "print(f'Test Loss: {avg_loss:.4f}, Test Accuracy: {acc:.4f}')\n",
    "\n"
   ],
   "id": "52d74e4cc70a6de0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.1211, Test Accuracy: 0.9661\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T21:33:42.090360Z",
     "start_time": "2025-11-15T21:33:41.881370Z"
    }
   },
   "cell_type": "code",
   "source": "torch.save(model.state_dict(), 'covid_classifier.pth')\n",
   "id": "2ed891393e15f7ec",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T21:48:53.623620Z",
     "start_time": "2025-11-15T21:48:53.413613Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.load_state_dict(torch.load('covid_classifier.pth'))\n",
    "model.eval()\n"
   ],
   "id": "cf61ee8d62636faf",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=2048, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e2f760894ce96c81"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
